{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LEAGUE (title tba)\n",
        "\n",
        "**Name(s)**: Palina Volskaya & Hieu Ngyuen\n",
        "\n",
        "**Website Link**: (your website link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-31T23:36:28.652554Z",
          "start_time": "2019-10-31T23:36:27.180520Z"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import plotly.express as px\n",
        "pd.options.plotting.backend = 'plotly'\n",
        "\n",
        "from dsc80_utils import * # Feel free to uncomment and use this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Introduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "league = pd.read_csv('data/LoL2022data.csv')\n",
        "league.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "teams = league[league['position']=='team']\n",
        "teams['firstdragon'] = teams['firstdragon'].astype(bool)\n",
        "players = league[league['position']!='team']\n",
        "teams.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "league_counts = teams['league'].value_counts() / 2\n",
        "league_counts = league_counts.reset_index()\n",
        "league_counts.columns = ['league', 'games_played']\n",
        "\n",
        "fig = px.bar(\n",
        "    league_counts,\n",
        "    x='league',\n",
        "    y='games_played',\n",
        "    title=\"Number of games played by league\",\n",
        "    labels={'league': 'League', 'games_played': 'Number of games'}\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "group_teams = teams.groupby('teamid')[['goldspent','result']].sum()\n",
        "fig = px.scatter(\n",
        "    group_teams,\n",
        "    x='goldspent',\n",
        "    y='result',\n",
        "    title=\"Plot of wins by a team against their total gold spent\",\n",
        "    labels={\n",
        "        'goldspent': 'Gold spent',\n",
        "        'result': 'Wins'\n",
        "    }\n",
        ")\n",
        "\n",
        "fig.update_traces(marker=dict(size=4, color='green'))\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yes_first_dragon = teams[teams['firstdragon']]\n",
        "no_first_dragon = teams[teams['firstdragon']==False]\n",
        "wins_yes = yes_first_dragon.groupby('teamid')['result'].sum()\n",
        "wins_no = no_first_dragon.groupby('teamid')['result'].sum()\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Histogram(\n",
        "    x=wins_yes,\n",
        "    name=\"With first dragon\",\n",
        "    marker=dict(color=\"blue\"),\n",
        "    opacity=0.75\n",
        "))\n",
        "\n",
        "fig.add_trace(go.Histogram(\n",
        "    x=wins_no,\n",
        "    name=\"Without first dragon\",\n",
        "    marker=dict(color=\"red\"),\n",
        "    opacity=0.7\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=\"Conditional distribution plot of team wins with or without first dragon\",\n",
        "    xaxis_title=\"Wins\",\n",
        "    yaxis_title=\"Frequency\",\n",
        "    barmode=\"overlay\"\n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "copy = teams.copy()\n",
        "result_arr = copy['kills']/copy['deaths']\n",
        "final_arr = np.where(np.isinf(result_arr), 0, result_arr)\n",
        "final_arr = np.nan_to_num(final_arr, nan=0, posinf=0, neginf=0)\n",
        "copy['kills per death'] = final_arr\n",
        "pivot_overall = pd.pivot_table(\n",
        "    copy,\n",
        "    values=['result','kills per death','goldspent'],\n",
        "    index='firstdragon',      \n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "pivot_overall = pivot_overall.rename_axis('firstdragon')\n",
        "pivot_overall = pivot_overall.rename(columns={\"result\":\"win percentage\"})\n",
        "pivot_overall['win percentage'] = (pivot_overall['win percentage']*100).round(2)\n",
        "pivot_overall['kills per death'] = (pivot_overall['kills per death']).round(2)\n",
        "pivot_overall['goldspent'] = (pivot_overall['goldspent']).round(2)\n",
        "pivot_overall.index = pivot_overall.index.map(\n",
        "    {True: 'Got first dragon', False: 'Did NOT get first dragon'}\n",
        ")\n",
        "pivot_overall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pivot_team = pd.pivot_table(\n",
        "    copy,\n",
        "    values='result',\n",
        "    index='teamname',           # one row per team\n",
        "    columns=['firstdragon'],    # two columns: 0 = no first drake, 1 = got first drake\n",
        "    aggfunc='mean'\n",
        ")\n",
        "\n",
        "# Convert to percentages & rename columns\n",
        "pivot_team = (pivot_team * 100).round(2)\n",
        "pivot_team = pivot_team.rename(\n",
        "    columns={\n",
        "        0: 'no_first_dragon_win_pct',\n",
        "        1: 'first_dragon_win_pct'\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Assessment of Missingness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After analyzing the dataset aggregated by teams, we found the `pick1` column to be NMAR due to its inability to be predicted from other columns, and the missingness potentially occuring due to a variable not included in the dataset. Unlike many of the columns we analyzed, the missingness of `pick1` did not depend on league, as many leagues didn't report certain variables as a whole. Instead, 31 out of the 55 leagues had missingness for `pick1` specifically. The missingness also did not depend on `teamid`, as 381 out of 593 teams had a missing value in `pick1`. Instead, the column's missingness depends on the value itself - the champion chosen. Missingness in this column can occur due to the champion being a newly added character to the game, resulting in a null value being recorded. Additional data that would make this column MAR would be data about whether the champion is a newly added character at the time of the game. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "observed_statistic = teams[teams['goldat25'].isna()]['gamelength'].mean()\n",
        "observed_means = []\n",
        "for i in range(1, 500):\n",
        "    permuted_gamelengths = teams.copy()\n",
        "    permuted_gamelengths['permuted_lengths'] = np.random.permutation(teams['gamelength'])\n",
        "    observed_means.append(permuted_gamelengths[permuted_gamelengths['goldat25'].isna()]['permuted_lengths'].mean())\n",
        "sum(observed_statistic >= observed_means) / 1000\n",
        "gamelength_plot = px.histogram(observed_means)\n",
        "gamelength_plot.add_vline(x=observed_statistic, line_dash=\"dash\", line_color=\"red\", line_width=2, opacity=1)\n",
        "gamelength_plot.update_layout(showlegend=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "observed_statistic = teams[teams['goldat25'].isna()]['firstdragon'].mean()\n",
        "observed_means = []\n",
        "for i in range(1, 500):\n",
        "    permuted_firstdragons = teams.copy()\n",
        "    permuted_firstdragons['permuted_lengths'] = np.random.permutation(teams['firstdragon'])\n",
        "    observed_means.append(permuted_firstdragons[permuted_firstdragons['goldat25'].isna()]['permuted_lengths'].mean())\n",
        "sum(observed_statistic >= observed_means) / 1000\n",
        "fig2 = px.histogram(observed_means)\n",
        "fig2.add_vline(x=observed_statistic, line_dash=\"dash\", line_color=\"red\", line_width=2, opacity=1)\n",
        "fig2.update_layout(showlegend=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Hypothesis Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-31T23:36:28.666489Z",
          "start_time": "2019-10-31T23:36:28.664381Z"
        }
      },
      "outputs": [],
      "source": [
        "dropped_columns = teams[['gameid','teamid','firstdragon','result']]\n",
        "N = 1000\n",
        "# Observed difference in proportions\n",
        "obs_diff = dropped_columns[dropped_columns['firstdragon'] == 1]['result'].mean() - \\\n",
        "           dropped_columns[dropped_columns['firstdragon'] == 0]['result'].mean()\n",
        "\n",
        "# Permutation test\n",
        "n_permutations = 10000\n",
        "diffs = np.zeros(n_permutations)\n",
        "\n",
        "for i in range(n_permutations):\n",
        "    shuffled = np.random.permutation(dropped_columns['result'])\n",
        "    diffs[i] = shuffled[dropped_columns['firstdragon'] == 1].mean() - \\\n",
        "               shuffled[dropped_columns['firstdragon'] == 0].mean()\n",
        "\n",
        "# Two-sided p-value\n",
        "p_value = np.sum(diffs >= obs_diff) / N\n",
        "fig = px.histogram(diffs,nbins=30)\n",
        "fig.add_vline(x=obs_diff, line_color='red', line_dash='dash', line_width=2)\n",
        "fig.add_annotation(\n",
        "    x=obs_diff,  \n",
        "    y=1,  \n",
        "    text=f'Observed diff = {obs_diff:.3f}',\n",
        "    yshift=100, \n",
        "    showarrow=False,\n",
        "    xshift=10,\n",
        "    textangle=-90, \n",
        ")\n",
        "fig.update_layout(showlegend=False, title=\"Permutation test for firstdragon\")\n",
        "fig.show()\n",
        "print(\"Observed difference:\", obs_diff)\n",
        "print(f\"Permutation p-value: {p_value:.3g}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Framing a Prediction Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-31T23:36:28.657068Z",
          "start_time": "2019-10-31T23:36:28.654650Z"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-31T23:36:28.662099Z",
          "start_time": "2019-10-31T23:36:28.660016Z"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier \n",
        "from sklearn.metrics import accuracy_score\n",
        "cols = teams.columns\n",
        "cols = [i for i in cols if '25' in i]\n",
        "cols.append('result')\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    teams[['goldat10','xpat10']], teams['result'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "pipe = Pipeline([       # preprocessing step\n",
        "    (\"clf\", DecisionTreeClassifier(\n",
        "    criterion=\"gini\",     # or \"entropy\", \"log_loss\"\n",
        "    max_depth=None,       # None = grow until pure / min_samples\n",
        "    random_state=42       # controls randomness (None = fully random each run)\n",
        "))\n",
        "])\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred = pipe.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "precision = y_pred.astype(int) @ y_test.astype(int) / y_pred.sum()\n",
        "recall = y_pred.astype(int) @ y_test.astype(int) / y_test.sum()\n",
        "print(f\"Test accuracy: {acc:.3f}\")\n",
        "print(f\"Test precision: {precision:.3f}\")\n",
        "print(f'Test recall: {recall:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-31T23:36:28.662099Z",
          "start_time": "2019-10-31T23:36:28.660016Z"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, KFold \n",
        "\n",
        "\n",
        "\n",
        "final= teams[cols].dropna(axis=0,how=\"any\")\n",
        "\n",
        "# Creating Pipeline \n",
        "\n",
        "pl = Pipeline([\n",
        " ('clf', RandomForestClassifier(\n",
        " random_state=42,\n",
        " n_jobs=-1,\n",
        " ))\n",
        "])\n",
        "n_features = X_train.shape[1]\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "hyperparameters = {\n",
        " 'clf__max_depth': [5],\n",
        " 'clf__n_estimators': [100],\n",
        " 'clf__max_features':list({\n",
        "        'log2',\n",
        "    }),\n",
        " 'clf__bootstrap': [True],\n",
        "}\n",
        "# hyperparameters = {\n",
        "#  'clf__max_depth': [None, 5, 10, 20],\n",
        "#  'clf__n_estimators': [100, 200, 300, 500],\n",
        "#  'clf__max_features':list({\n",
        "#         'sqrt',\n",
        "#         'log2',\n",
        "#     }),\n",
        "#  'clf__bootstrap': [True, False],\n",
        "# }\n",
        "grids = GridSearchCV(\n",
        " pl,\n",
        " n_jobs=-1, \n",
        " param_grid=hyperparameters,\n",
        " return_train_score=True,\n",
        " cv=kf,\n",
        " scoring=\"f1\",\n",
        " verbose=2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "final_no_result = final[[i for i in cols if i !=\"result\"]]\n",
        "final_result = final['result']\n",
        "\n",
        "def make_quadratic(df):\n",
        "    combinations =itertools.combinations(df.columns,r=2)\n",
        "    return_dict = {}\n",
        "    for combination in combinations:\n",
        "        return_dict[f'{combination[0]}_{combination[1]}'] = df[combination[0]]*df[combination[1]]\n",
        "    return pd.DataFrame(return_dict)\n",
        "\n",
        "fe_final = make_quadratic(final_no_result)\n",
        "X_train_fe, X_test_fe, y_train_fe, y_test_fe = train_test_split(\n",
        "    fe_final,final_result, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "grids.fit(X_train_fe, y_train_fe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = grids.predict(X_test_fe)\n",
        "precision = predictions.astype(int) @ y_test_fe.astype(int) / predictions.sum()\n",
        "recall = predictions.astype(int) @ y_test_fe.astype(int) / y_test_fe.sum()\n",
        "print(\"Best params:\", grids.best_params_)\n",
        "print(\"Best CV score:\", grids.best_score_)\n",
        "print(\"Test score:\", grids.score(X_test_fe, y_test_fe))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Fairness Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-31T23:36:28.666489Z",
          "start_time": "2019-10-31T23:36:28.664381Z"
        }
      },
      "outputs": [],
      "source": [
        "leagues = teams['league'].unique()\n",
        "dataframe = {}\n",
        "\n",
        "for league in leagues:\n",
        "    filtered_no_results = teams[teams['league']==league][[i for i in cols if i !='result']]\n",
        "    filtered_results = teams[teams['league']==league]['result']\n",
        "    filtered_final = make_quadratic(filtered_no_results)\n",
        "    predictions = grids.predict(filtered_final)\n",
        "    F_1 = grids.score(filtered_final,filtered_results)\n",
        "    \n",
        "    dataframe[league] = {\"F_1\": F_1}\n",
        "\n",
        "split_dataframe = pd.DataFrame(dataframe).T\n",
        "split_dataframe.plot(y='F_1',kind='bar')\n",
        "fig = px.bar(split_dataframe, barmode='group')\n",
        "fig.update_layout(title=\"F-1 scores for each league\", yaxis_title=\"rate\", xaxis_title=\"league\")\n",
        "fig.show()\n",
        "fig.write_html(\"Side-by-side_precision_for_each_league\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LCL = teams[teams['league']=='LCL'][[i for i in cols if i !='result']]\n",
        "LCL_results = teams[teams['league']=='LCL']['result']\n",
        "observed_df = make_quadratic(LCL)\n",
        "predictions = grids.predict(observed_df)\n",
        "observed_f1 = grids.score(observed_df,LCL_results)\n",
        "\n",
        "N = 1000\n",
        "f1s = []\n",
        "for i in range(N):\n",
        "    copy = teams.copy()\n",
        "    copy['league'] = np.random.permutation(teams['league'])\n",
        "    LCL_permute = copy[copy['league']=='LCL'][[i for i in cols if i !='result']]\n",
        "    LCL_results_permute = copy[copy['league']=='LCL']['result']\n",
        "    test_df = make_quadratic(LCL_permute)\n",
        "    predictions = grids.predict(test_df)\n",
        "    test_f1 = grids.score(test_df,LCL_results_permute)\n",
        "    f1s.append(test_f1)\n",
        "fig = px.histogram(f1s, nbins=12)\n",
        "fig.add_vline(x=observed_f1,line_width=2,line_dash=\"dash\",line_color=\"red\", opacity=1)\n",
        "fig.update_layout(showlegend=False, \n",
        "                  title_text=\"Distribution of model precision\",\n",
        "                  title_x=0.5,  # Center the title\n",
        "                  title_xanchor='center')\n",
        "fig.show()\n",
        "fig.write_html('Distribution of Model Precision')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_value = (f1s>=observed_f1).sum() / len(f1s)\n",
        "print(f'p_value: {p_value:.3g}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dsc80",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
